data:
  data_dir: 'datasets/ljspeech/data/'
  wav_dir: 'datasets/ljspeech/audio/'
  # Compute statistics
  e_mean: 21.578571319580078
  e_std: 18.916799545288086
  e_min: 0.01786651276051998
  e_max: 130.5338592529297

  f0_mean: 206.5135564772342
  f0_std:  53.633228905750336
  p_min: 71.0
  p_max: 676.2260946528305 # 799.8901977539062
  train_filelist: "datasets/ljspeech/train_filelist.txt"
  valid_filelist: "datasets/ljspeech/valid_filelist.txt"
  tts_cleaner_names: ['english_cleaners']

# feature extraction related
audio:
  sample_rate: 22050      # sampling frequency
  fmax: 8000.0       # maximum frequency
  fmin: 0.0       # minimum frequency
  n_mels: 80     # number of mel basis
  n_fft: 1024    # number of fft points
  hop_length: 256   # number of shift points
  win_length: 1024 # window length
  num_mels : 80
  min_level_db : -100
  ref_level_db : 20
  bits : 9                            # bit depth of signal
  mu_law : True                       # Recommended to suppress noise if using raw bits in hp.voc_mode below
  peak_norm : False                   # Normalise to the peak of each wav file


model:
  config_folder: "configs/optispeech/generator"
  config_name: default.yaml
  transformer_warmup_steps: 4000
  transformer_lr: 1.0

train:
  # optimization related
  eos: False #True
  opt: 'noam'
  accum_grad: 4
  grad_clip: 1.0
  weight_decay: 0.001
  patience: 0
  epochs: 1000  # 1,000 epochs * 809 batches / 5 accum_grad : 161,800 iters
  save_interval_epoch: 10
  GTA : False
  # other
  ngpu: 1       # number of gpus ("0" uses cpu, otherwise use gpu)
  nj: 4        # number of parallel jobs
  dumpdir: '' # directory to dump full features
  verbose: 0    # verbose option (if set > 0, get more log)
  N: 0          # number of minibatches to be used (mainly for debugging). "0" uses all minibatches.
  seed: 1       # random seed number
  resume: ""    # the snapshot path to resume (if set empty, no effect)
  use_phonemes: True
  batch_size : 16
  # other
  melgan_vocoder : True
  save_interval : 1000
  chkpt_dir : './checkpoints'
  log_dir : './logs'
  summary_interval : 200
  validation_step : 500
  tts_max_mel_len : 870              # if you have a couple of extremely long spectrograms you might want to use this
  tts_bin_lengths : True              # bins the spectrogram lengths before sampling in data loader - speeds up training
